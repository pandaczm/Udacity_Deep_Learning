{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x110f10090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = mnist.train.images[7]\n",
    "b = mnist.train.images[2]\n",
    "a = a.reshape(28,28)\n",
    "b = b.reshape(28,28)\n",
    "c = np.zeros((28,28))\n",
    "d = np.concatenate((a,b,c),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (10000, 784) (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.images.shape,mnist.test.images.shape,mnist.validation.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAADGCAYAAAA3xDJsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11519c890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Generate_dataset(original_dataset, dataset_size, max_length):\n",
    "    labels_list = list()\n",
    "    data_list = list()\n",
    "    original_size = original_dataset.images.shape[0]\n",
    "    for i in range(max_length):\n",
    "        labels_list.append([])\n",
    "    for i in range(dataset_size):\n",
    "        num_length = random.randint(1, max_length)\n",
    "        tmp = list()\n",
    "        for j in range(max_length):\n",
    "            if j < num_length:\n",
    "                index = random.randint(0, original_size - 1)\n",
    "#                 print index\n",
    "                tmp.append(original_dataset.images[index].reshape(28,28))\n",
    "                data_label = np.concatenate((original_dataset.labels[index], np.array([0])), axis = 0)\n",
    "                labels_list[j].append(data_label.tolist())\n",
    "                \n",
    "            else:\n",
    "                tmp.append(np.zeros((28,28)))\n",
    "                data_label = np.concatenate((np.zeros(10), np.array([1])), axis=0)\n",
    "                labels_list[j].append(data_label.tolist())\n",
    "        data_list.append(np.concatenate(tmp, axis = 1).reshape((1, 28 * 28 * max_length)))\n",
    "    labels_list = [np.array(i) for i in labels_list]\n",
    "        \n",
    "    return np.concatenate(data_list, axis = 0), labels_list\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(num_length):\n",
    "    label_num = [0,0,0,0,0]\n",
    "    label_num[num_length - 1] = 1\n",
    "    return label_num\n",
    "\n",
    "def Generate_dataset_1(original_dataset, dataset_size, max_length):\n",
    "    labels_list = list()\n",
    "    data_list = list()\n",
    "    labels = list()\n",
    "    label_seqno = list()\n",
    "    original_size = original_dataset.images.shape[0]\n",
    "    for i in range(max_length + 1):\n",
    "        labels_list.append([])\n",
    "    for i in range(dataset_size):\n",
    "        num_length = random.randint(1, max_length)\n",
    "        label_seqno.append(num_length)\n",
    "        labels_list[0].append(one_hot(num_length))\n",
    "        tmp = list()\n",
    "        label_tmp = []\n",
    "        for j in range(max_length):\n",
    "            if j < num_length:\n",
    "                index = random.randint(0, original_size - 1)\n",
    "#                 print index\n",
    "                tmp.append(original_dataset.images[index].reshape(28,28))\n",
    "                data_label = original_dataset.labels[index]\n",
    "                labels_list[j + 1].append(data_label.tolist())\n",
    "                label_tmp.append(np.argmax(data_label).tolist())\n",
    "                \n",
    "            else:\n",
    "                tmp.append(np.zeros((28,28)))\n",
    "                data_label = np.zeros(10)\n",
    "                labels_list[j + 1].append(data_label.tolist())\n",
    "                label_tmp.append(11)\n",
    "        data_list.append(np.concatenate(tmp, axis = 1).reshape((1, 28 * 28 * max_length)))\n",
    "        labels.append(label_tmp)\n",
    "    labels_list = [np.array(i) for i in labels_list]\n",
    "        \n",
    "    return np.concatenate(data_list, axis = 0), labels_list, np.array(labels), np.array(label_seqno)\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset, train_labels = Generate_dataset(mnist.train,55000,5)\n",
    "valid_dataset, valid_labels = Generate_dataset(mnist.validation,5000,5)\n",
    "test_dataset, test_labels = Generate_dataset(mnist.test,10000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset_ref, train_labels_ref, t_l,t_n = Generate_dataset_1(mnist.train,55000,5)\n",
    "valid_dataset_ref, valid_labels_ref,v_l,v_n = Generate_dataset_1(mnist.validation,5000,5)\n",
    "test_dataset_ref, test_labels_ref,te_l,te_n = Generate_dataset_1(mnist.test,10000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test =train_dataset_ref[1].reshape(28,140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d257990>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACJCAYAAADJyorVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nT0lJSUlJOQWkgp+SkpKSknIK+P8By/B/UkM96ZIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114f10a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_n[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'sequence_ref.pickle'\n",
    "try:\n",
    "  with open(pickle_file, 'wb') as f:\n",
    "      save = {\n",
    "        'train_dataset_ref': train_dataset_ref,\n",
    "        'train_labels_ref': train_labels_ref,\n",
    "        'valid_dataset_ref': valid_dataset_ref,\n",
    "        'valid_labels_ref': valid_labels_ref,\n",
    "        'test_dataset_ref': test_dataset_ref,\n",
    "        'test_labels_ref': test_labels_ref,\n",
    "        }\n",
    "      pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'sequence.pickle'\n",
    "try:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        dataset_dict = pickle.load(f)\n",
    "        train_dataset = dataset_dict['train_dataset']\n",
    "        test_dataset = dataset_dict['test_dataset']\n",
    "        train_labels = dataset_dict['train_labels']\n",
    "        valid_dataset = dataset_dict['valid_dataset']\n",
    "        valid_labels = dataset_dict['valid_labels']\n",
    "        test_labels = dataset_dict['test_labels']\n",
    "    \n",
    "except Exception as e:\n",
    "    print ('Unable to load the data')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (55000, 28, 140, 1))\n",
      "('Validation set', (5000, 28, 140, 1))\n",
      "('Test set', (10000, 28, 140, 1))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size * 5, num_channels)).astype(np.float32)\n",
    "  return dataset\n",
    "train_dataset = reformat(train_dataset, train_labels)\n",
    "valid_dataset = reformat(valid_dataset, valid_labels)\n",
    "test_dataset = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape)\n",
    "print('Validation set', valid_dataset.shape)\n",
    "print('Test set', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "test = test_dataset[100]\n",
    "print test_labels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "max_length = 5\n",
    "num_labels = 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "def accuracy(predictions, labels):\n",
    "    accur = 0\n",
    "    for index, item in enumerate(predictions):\n",
    "        accur += np.sum(np.argmax(item, 1) == np.argmax(labels[index], 1))\n",
    "#         print accur\n",
    "#         print item.shape\n",
    "    return (100.0 * accur / float(len(predictions) * predictions[0].shape[0]))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size * 5, num_channels))\n",
    "    tf_train_labels = list()\n",
    "    for _ in range(max_length):\n",
    "        tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, num_labels)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_variable([depth])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_variable([depth])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_variable([num_hidden])\n",
    "    s1_w = weight_variable([num_hidden,num_labels])\n",
    "    s1_b = bias_variable([num_labels])\n",
    "    s2_w = weight_variable([num_hidden,num_labels])\n",
    "    s2_b = bias_variable([num_labels])\n",
    "    s3_w = weight_variable([num_hidden,num_labels])\n",
    "    s3_b = bias_variable([num_labels])\n",
    "    s4_w = weight_variable([num_hidden,num_labels])\n",
    "    s4_b = bias_variable([num_labels])\n",
    "    s5_w = weight_variable([num_hidden,num_labels])\n",
    "    s5_b = bias_variable([num_labels])\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits1 = tf.matmul(hidden, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(hidden, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(hidden, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(hidden, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(hidden, s5_w) + s5_b\n",
    "\n",
    "        return [logits1,logits2,logits3,logits4,logits5]\n",
    "    \n",
    "    # Training computation.\n",
    "    logits= model(tf_train_dataset)\n",
    "    logits_train = model(tf_train_dataset,False)\n",
    "    logits_valid = model(tf_valid_dataset, False)\n",
    "    logits_test = model(tf_test_dataset, False)\n",
    "    loss = 0\n",
    "    for i in range(max_length):\n",
    "        loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i], logits=logits[i])) \n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "\n",
    "    train_prediction = tf.stack((tf.nn.softmax(logits_train[0]), tf.nn.softmax(logits_train[1]), tf.nn.softmax(logits_train[2]), \\\n",
    "                                 tf.nn.softmax(logits_train[3]), tf.nn.softmax(logits_train[4])))\n",
    "    valid_prediction = tf.stack((tf.nn.softmax(logits_valid[0]), tf.nn.softmax(logits_valid[1]), tf.nn.softmax(logits_valid[2]),tf.nn.softmax(logits_valid[3]), tf.nn.softmax(logits_valid[4])))\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 31.628092\n",
      "Minibatch accuracy: 15.0%\n",
      "Validation accuracy: 21.7%\n",
      "Minibatch loss at step 50: 7.836520\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 100: 7.846429\n",
      "Minibatch accuracy: 45.0%\n",
      "Validation accuracy: 45.3%\n",
      "Minibatch loss at step 150: 7.362939\n",
      "Minibatch accuracy: 46.2%\n",
      "Minibatch loss at step 200: 9.750185\n",
      "Minibatch accuracy: 30.0%\n",
      "Validation accuracy: 47.2%\n",
      "Minibatch loss at step 250: 7.382410\n",
      "Minibatch accuracy: 48.8%\n",
      "Minibatch loss at step 300: 8.045277\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 51.8%\n",
      "Minibatch loss at step 350: 5.696900\n",
      "Minibatch accuracy: 67.5%\n",
      "Minibatch loss at step 400: 6.201841\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 55.8%\n",
      "Minibatch loss at step 450: 5.995695\n",
      "Minibatch accuracy: 58.8%\n",
      "Minibatch loss at step 500: 4.603451\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 55.4%\n",
      "Minibatch loss at step 550: 6.294787\n",
      "Minibatch accuracy: 65.0%\n",
      "Minibatch loss at step 600: 7.287162\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 58.2%\n",
      "Minibatch loss at step 650: 7.350429\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 700: 7.531975\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 60.5%\n",
      "Minibatch loss at step 750: 6.983850\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 800: 6.750903\n",
      "Minibatch accuracy: 66.2%\n",
      "Validation accuracy: 62.6%\n",
      "Minibatch loss at step 850: 8.291512\n",
      "Minibatch accuracy: 58.8%\n",
      "Minibatch loss at step 900: 7.300445\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 63.5%\n",
      "Minibatch loss at step 950: 6.689168\n",
      "Minibatch accuracy: 63.8%\n",
      "Minibatch loss at step 1000: 6.082296\n",
      "Minibatch accuracy: 61.2%\n",
      "Validation accuracy: 67.6%\n",
      "Minibatch loss at step 1050: 6.287464\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 1100: 6.359310\n",
      "Minibatch accuracy: 71.2%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 1150: 7.268593\n",
      "Minibatch accuracy: 58.8%\n",
      "Minibatch loss at step 1200: 6.484223\n",
      "Minibatch accuracy: 66.2%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 1250: 5.922056\n",
      "Minibatch accuracy: 66.2%\n",
      "Minibatch loss at step 1300: 6.720294\n",
      "Minibatch accuracy: 66.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 1350: 6.698498\n",
      "Minibatch accuracy: 72.5%\n",
      "Minibatch loss at step 1400: 4.791230\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 1450: 6.683983\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 1500: 5.868049\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 1550: 4.955076\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 1600: 4.030553\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 1650: 7.840498\n",
      "Minibatch accuracy: 67.5%\n",
      "Minibatch loss at step 1700: 5.192934\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 1750: 5.599350\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 1800: 3.944980\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 1850: 5.647645\n",
      "Minibatch accuracy: 73.8%\n",
      "Minibatch loss at step 1900: 6.209970\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1950: 5.516533\n",
      "Minibatch accuracy: 76.2%\n",
      "Minibatch loss at step 2000: 5.297920\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2050: 5.173547\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 2100: 5.953873\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2150: 5.173934\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 2200: 5.278087\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2250: 8.384549\n",
      "Minibatch accuracy: 65.0%\n",
      "Minibatch loss at step 2300: 5.285096\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2350: 4.565937\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 2400: 4.213479\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2450: 4.815008\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 2500: 4.271991\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2550: 5.175226\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 2600: 6.273985\n",
      "Minibatch accuracy: 73.8%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2650: 4.020210\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 2700: 5.474651\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2750: 5.473088\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 2800: 5.448801\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 2850: 6.808837\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2900: 5.319247\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2950: 5.163090\n",
      "Minibatch accuracy: 82.5%\n",
      "Minibatch loss at step 3000: 4.404227\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 3050: 4.308691\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 3100: 4.060828\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3150: 4.581129\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 3200: 5.179672\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 3250: 6.318537\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 3300: 4.680129\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3350: 4.456341\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 3400: 2.939851\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 3450: 4.107376\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3500: 4.477906\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3550: 4.186306\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 3600: 5.310297\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 3650: 5.400830\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 3700: 5.427987\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 3750: 4.746089\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 3800: 3.791587\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 3850: 6.117607\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 3900: 3.841712\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3950: 3.345016\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 4000: 4.621937\n",
      "Minibatch accuracy: 78.8%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 4050: 4.597626\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 4100: 5.165287\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 4150: 4.402647\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 4200: 4.622455\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4250: 4.047225\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 4300: 4.954624\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4350: 4.533694\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 4400: 5.523603\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4450: 4.599563\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 4500: 4.499293\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4550: 4.621846\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 4600: 4.124416\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 4650: 4.011474\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 4700: 4.842143\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 4750: 4.661561\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 4800: 3.691018\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 4850: 3.277869\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4900: 4.135070\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 4950: 3.129409\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 5000: 4.851027\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 5050: 3.400196\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 5100: 4.348362\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5150: 4.134158\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 5200: 3.808074\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 5250: 4.053674\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 5300: 3.491196\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 5350: 5.074876\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 5400: 3.122880\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 5450: 4.532822\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 5500: 6.158643\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 5550: 4.621519\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 5600: 4.978096\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 5650: 4.049908\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 5700: 3.118755\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 5750: 4.884913\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 5800: 4.443269\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 5850: 4.502300\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 5900: 6.397976\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 5950: 4.363233\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 6000: 4.500070\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 6050: 3.632603\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 6100: 2.958556\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 6150: 4.640641\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 6200: 3.622876\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.6%\n",
      "Minibatch loss at step 6250: 2.989401\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 6300: 3.535127\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 6350: 4.725656\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 6400: 3.215297\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 91.7%\n",
      "Minibatch loss at step 6450: 4.513044\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 6500: 3.345408\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 91.8%\n",
      "Minibatch loss at step 6550: 4.969257\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 6600: 4.353186\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 6650: 4.659258\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 6700: 3.000465\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 92.3%\n",
      "Minibatch loss at step 6750: 3.498826\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 6800: 4.345128\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 6850: 5.284641\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 6900: 4.118309\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 92.2%\n",
      "Minibatch loss at step 6950: 4.418940\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 7000: 3.554558\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 92.1%\n",
      "Minibatch loss at step 7050: 4.354369\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 7100: 3.189608\n",
      "Minibatch accuracy: 98.8%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 7150: 2.263660\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 7200: 3.978348\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 92.6%\n",
      "Minibatch loss at step 7250: 3.962702\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 7300: 3.921335\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 7350: 4.474620\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 7400: 4.236339\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.7%\n",
      "Minibatch loss at step 7450: 4.683732\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 7500: 4.232737\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 92.5%\n",
      "Minibatch loss at step 7550: 3.717525\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 7600: 5.029587\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 7650: 4.106392\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 7700: 3.914740\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 7750: 4.416513\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 7800: 4.801089\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 7850: 4.073766\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 7900: 3.783421\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.0%\n",
      "Minibatch loss at step 7950: 3.209636\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 8000: 3.877208\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 8050: 3.439304\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 8100: 3.284340\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 92.9%\n",
      "Minibatch loss at step 8150: 4.609380\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 8200: 4.117509\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 8250: 4.014759\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 8300: 4.634562\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.2%\n",
      "Minibatch loss at step 8350: 2.598963\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 8400: 3.605269\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 8450: 4.565663\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 8500: 2.544050\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 8550: 3.006317\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 8600: 3.352355\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 8650: 4.118413\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 8700: 3.469291\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 8750: 3.770175\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 8800: 3.577480\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 8850: 3.991931\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 8900: 3.377398\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 93.6%\n",
      "Minibatch loss at step 8950: 4.282163\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 9000: 4.641809\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 9050: 2.972398\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 9100: 2.705298\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 9150: 4.313028\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 9200: 2.171080\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.7%\n",
      "Minibatch loss at step 9250: 2.521854\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 9300: 2.383067\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 9350: 3.247345\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 9400: 3.016637\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 9450: 4.702148\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 9500: 4.494668\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 9550: 2.520921\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 9600: 4.283316\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 9650: 3.106000\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 9700: 4.390415\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 9750: 3.133137\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 9800: 2.862700\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 93.8%\n",
      "Minibatch loss at step 9850: 4.387556\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 9900: 6.085245\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 93.9%\n",
      "Minibatch loss at step 9950: 3.449317\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 10000: 4.729020\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 94.1%\n",
      "Test accuracy: 94.4%\n",
      "Model saved in file: CNN_multi.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "    feed_dict = dict()\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    feed_dict[tf_train_dataset] = batch_data\n",
    "    batch_labels = list()\n",
    "    for i in range(max_length):\n",
    "        batch_labels.append(train_labels[i][offset:(offset + batch_size), :])\n",
    "        feed_dict[tf_train_labels[i]] = train_labels[i][offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "    if (step % 100 == 0):\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  save_path = saver.save(session, \"CNN_multi.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 140, 1)\n"
     ]
    }
   ],
   "source": [
    "print test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_test = []\n",
    "for i in range(5):\n",
    "    sample_test_index = random.randint(0,9999)\n",
    "    sample_test.append(sample_test_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample = test_dataset[sample_test,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (5, image_size, image_size * 5, num_channels))\n",
    " \n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_variable([depth])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_variable([depth])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_variable([num_hidden])\n",
    "    s1_w = weight_variable([num_hidden,num_labels])\n",
    "    s1_b = bias_variable([num_labels])\n",
    "    s2_w = weight_variable([num_hidden,num_labels])\n",
    "    s2_b = bias_variable([num_labels])\n",
    "    s3_w = weight_variable([num_hidden,num_labels])\n",
    "    s3_b = bias_variable([num_labels])\n",
    "    s4_w = weight_variable([num_hidden,num_labels])\n",
    "    s4_b = bias_variable([num_labels])\n",
    "    s5_w = weight_variable([num_hidden,num_labels])\n",
    "    s5_b = bias_variable([num_labels])\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits1 = tf.matmul(hidden, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(hidden, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(hidden, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(hidden, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(hidden, s5_w) + s5_b\n",
    "\n",
    "        return [logits1,logits2,logits3,logits4,logits5]\n",
    "    \n",
    "    # Training computation.\n",
    "    logits= model(tf_test_dataset)\n",
    "   \n",
    "    # Predictions for the training, validation, and test data.\n",
    "\n",
    "    logits_test = model(tf_test_dataset, False)\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    test_predict_labels = tf.transpose(tf.argmax(test_prediction, 2))\n",
    "\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Initialized\n",
      "[[ 7  3 10 10 10]\n",
      " [ 7  8 10 10 10]\n",
      " [ 9  4  3  9 10]\n",
      " [ 9  9  1  0  3]\n",
      " [ 0  6 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"/Users/pandaczm/PhD_Projects/GitHub/Udacity_Deep_Learning/Final_Project/CNN_multi.ckpt\")\n",
    "  print(\"Model restored.\")  \n",
    "\n",
    "  print('Initialized')\n",
    "  test_prediction = session.run(test_predict_labels, feed_dict={tf_test_dataset : test_sample})\n",
    "  print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_get_variable(name, shape):\n",
    "    weight = tf.get_variable(name,shape,initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    return weight\n",
    "\n",
    "def bias_get_variable(name,shape):\n",
    "    bias = tf.get_variable(name,shape,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits0/l1_b:0\n",
      "logits0/l1_b:0\n",
      "logits0/l1_b:0\n",
      "logits0/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits4/l1_b:0\n",
      "logits4/l1_b:0\n",
      "logits4/l1_b:0\n",
      "logits4/l1_b:0\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size * 5, num_channels))\n",
    "    tf_train_labels = list()\n",
    "    for _ in range(max_length):\n",
    "        tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, num_labels)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset[:2000])\n",
    "    \n",
    "    # Variables.\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        layer1_weights = weight_get_variable('l1_w',[patch_size, patch_size, num_channels, depth])\n",
    "        layer1_bias = bias_get_variable('l1_b',[depth])\n",
    "        layer2_weights = weight_get_variable('l2_w',[patch_size, patch_size, depth, depth])\n",
    "        layer2_bias = bias_get_variable('l2_b',[depth])\n",
    "        layer3_weights = weight_get_variable('l3_w',[image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "        layer3_bias = bias_get_variable('l3_b',[num_hidden])\n",
    "        s_w = weight_get_variable('f_w', [num_hidden,num_labels])\n",
    "        s_b = bias_get_variable('f_b',[num_labels])\n",
    "\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        print layer1_bias.name\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits = tf.matmul(hidden, s_w) + s_b\n",
    "\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = []\n",
    "    logits_train = []\n",
    "    logits_test = []\n",
    "    logits_valid = []\n",
    "    with tf.variable_scope('logits0') as scope_1:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_1.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits1') as scope_2:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_2.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits2') as scope_3:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_3.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits3') as scope_4:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_4.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits4') as scope_5:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_5.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    loss = 0\n",
    "    for i in range(max_length):\n",
    "        loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i], logits=logits[i])) \n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "\n",
    "    train_prediction = tf.stack((tf.nn.softmax(logits_train[0]), tf.nn.softmax(logits_train[1]), tf.nn.softmax(logits_train[2]), \\\n",
    "                                 tf.nn.softmax(logits_train[3]), tf.nn.softmax(logits_train[4])))\n",
    "    valid_prediction = tf.stack((tf.nn.softmax(logits_valid[0]), tf.nn.softmax(logits_valid[1]), tf.nn.softmax(logits_valid[2]),tf.nn.softmax(logits_valid[3]), tf.nn.softmax(logits_valid[4])))\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.265629\n",
      "Minibatch accuracy: 3.8%\n",
      "Validation accuracy: 33.1%\n",
      "Minibatch loss at step 50: 7.827024\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 100: 7.709940\n",
      "Minibatch accuracy: 41.2%\n",
      "Minibatch loss at step 150: 7.059905\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 200: 9.057103\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 250: 7.078635\n",
      "Minibatch accuracy: 58.8%\n",
      "Minibatch loss at step 300: 6.898920\n",
      "Minibatch accuracy: 60.0%\n",
      "Minibatch loss at step 350: 5.031794\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 400: 5.242235\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 450: 4.755205\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 500: 3.641416\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 550: 4.422575\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 600: 5.102674\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 650: 4.711769\n",
      "Minibatch accuracy: 76.2%\n",
      "Minibatch loss at step 700: 4.316792\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 750: 4.756740\n",
      "Minibatch accuracy: 73.8%\n",
      "Minibatch loss at step 800: 4.178140\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 850: 5.304820\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 900: 4.732872\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 950: 3.924833\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1000: 3.861768\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1050: 3.389151\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 1100: 4.181420\n",
      "Minibatch accuracy: 82.5%\n",
      "Minibatch loss at step 1150: 3.639707\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 1200: 3.955760\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1250: 3.872199\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 1300: 4.280478\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 1350: 4.299892\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1400: 2.398077\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 1450: 3.723983\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 1500: 3.747082\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 1550: 2.093361\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 1600: 3.178032\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 1650: 4.118211\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 1700: 3.693977\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 1750: 2.481850\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 1800: 2.572172\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 1850: 3.004564\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 1900: 2.333255\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 1950: 3.566700\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2000: 2.235529\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 2050: 2.061288\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2100: 2.770049\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 2150: 3.114858\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2200: 2.977314\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 2250: 3.831862\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2300: 2.830133\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2350: 2.688179\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 2400: 2.345165\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 2450: 2.267612\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 2500: 2.098443\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 2550: 2.405329\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2600: 2.409330\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 2650: 1.622021\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2700: 2.937958\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 2750: 2.057053\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 2800: 2.603288\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 2850: 3.022257\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 2900: 3.043956\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2950: 2.579618\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 3000: 1.959378\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 94.6%\n",
      "Minibatch loss at step 3050: 1.254169\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 3100: 1.304604\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3150: 1.609709\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3200: 2.052211\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 3250: 2.796769\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3300: 2.424225\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 3350: 1.528197\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3400: 1.566339\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 3450: 1.215003\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 3500: 2.037157\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 95.1%\n",
      "Minibatch loss at step 3550: 2.086021\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3600: 1.721193\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3650: 1.837015\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3700: 1.694475\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3750: 1.211546\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 3800: 1.561422\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 3850: 2.095030\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3900: 1.043255\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 3950: 1.159370\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4000: 1.826863\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 95.5%\n",
      "Minibatch loss at step 4050: 1.400965\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4100: 2.203825\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4150: 1.633132\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4200: 1.611597\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 4250: 1.903408\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4300: 1.274378\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4350: 2.382059\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4400: 1.517017\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4450: 1.407132\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4500: 1.586271\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 96.1%\n",
      "Minibatch loss at step 4550: 1.325540\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4600: 2.023803\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4650: 1.659499\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4700: 1.271218\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4750: 2.150717\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 4800: 1.673336\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4850: 1.015289\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4900: 1.518059\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4950: 0.931153\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 5000: 1.380117\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 96.4%\n",
      "Test accuracy: 96.8%\n",
      "Model saved in file: CNN_multi_different.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "    feed_dict = dict()\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    feed_dict[tf_train_dataset] = batch_data\n",
    "    batch_labels = list()\n",
    "    for i in range(max_length):\n",
    "        batch_labels.append(train_labels[i][offset:(offset + batch_size), :])\n",
    "        feed_dict[tf_train_labels[i]] = train_labels[i][offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "    if (step % 500 == 0):\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), [test_labels[0][:2000],test_labels[1][:2000], test_labels[2][:2000], test_labels[3][:2000], test_labels[4][:2000]]))\n",
    "  save_path = saver.save(session, \"CNN_multi_different.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits0/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits4/l1_b:0\n",
      "Model restored\n",
      "Initialized\n",
      "[[ 3  1  5  7 10]\n",
      " [ 2  5  9  2  7]\n",
      " [ 9 10 10 10 10]\n",
      " [ 3  8  6  9  6]\n",
      " [ 8 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "\n",
    "tf_test_dataset = tf.placeholder(tf.float32, shape = (5, image_size, image_size * 5, num_channels))\n",
    "\n",
    "# Variables.\n",
    "def model(data,train_only = True):\n",
    "    layer1_weights = weight_get_variable('l1_w',[patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_get_variable('l1_b',[depth])\n",
    "    layer2_weights = weight_get_variable('l2_w',[patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_get_variable('l2_b',[depth])\n",
    "    layer3_weights = weight_get_variable('l3_w',[image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_get_variable('l3_b',[num_hidden])\n",
    "    s_w = weight_get_variable('f_w', [num_hidden,num_labels])\n",
    "    s_b = bias_get_variable('f_b',[num_labels])\n",
    "\n",
    "    conv = conv2d(data, layer1_weights)\n",
    "    hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "    print layer1_bias.name\n",
    "    h_pool_1 = max_pool_2x2(hidden_1)\n",
    "    if train_only:\n",
    "        h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "    conv = conv2d(h_pool_1, layer2_weights)\n",
    "    hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "    h_pool_2 = max_pool_2x2(hidden_2)\n",
    "    if train_only:\n",
    "        h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "    shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "    reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "    if train_only:\n",
    "        hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "    logits = tf.matmul(hidden, s_w) + s_b\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "\n",
    "# Training computation.\n",
    "\n",
    "logits_test = []\n",
    "\n",
    "with tf.variable_scope('logits0') as scope_1:\n",
    "    scope_1.reuse_variables()\n",
    "\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "with tf.variable_scope('logits1') as scope_2:\n",
    "    scope_2.reuse_variables()\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "with tf.variable_scope('logits2') as scope_3:\n",
    "    scope_3.reuse_variables()\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "with tf.variable_scope('logits3') as scope_4:\n",
    "    scope_4.reuse_variables()\n",
    "\n",
    "\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "with tf.variable_scope('logits4') as scope_5:\n",
    "    scope_5.reuse_variables()\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "\n",
    "test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "test_predict_labels = tf.transpose(tf.argmax(test_prediction, 2))\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "session = tf.Session()\n",
    "saver.restore(session,\"/Users/pandaczm/PhD_Projects/GitHub/Udacity_Deep_Learning/Final_Project/CNN_multi_different.ckpt\")\n",
    "print ('Model restored')\n",
    "print('Initialized')\n",
    "test_prediction = session.run(test_predict_labels, feed_dict={tf_test_dataset : test_sample})\n",
    "print(test_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits0/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits4/l1_b:0\n",
      "Model restored\n",
      "Initialized\n",
      "[[ 3  1  5  7 10]\n",
      " [ 2  5  9  2  7]\n",
      " [ 9 10 10 10 10]\n",
      " [ 3  8  6  9  6]\n",
      " [ 8 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (5, image_size, image_size * 5, num_channels))\n",
    "\n",
    "    # Variables.\n",
    "    def model(data,train_only = True):\n",
    "        layer1_weights = weight_get_variable('l1_w',[patch_size, patch_size, num_channels, depth])\n",
    "        layer1_bias = bias_get_variable('l1_b',[depth])\n",
    "        layer2_weights = weight_get_variable('l2_w',[patch_size, patch_size, depth, depth])\n",
    "        layer2_bias = bias_get_variable('l2_b',[depth])\n",
    "        layer3_weights = weight_get_variable('l3_w',[image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "        layer3_bias = bias_get_variable('l3_b',[num_hidden])\n",
    "        s_w = weight_get_variable('f_w', [num_hidden,num_labels])\n",
    "        s_b = bias_get_variable('f_b',[num_labels])\n",
    "\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        print layer1_bias.name\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "    #         print type(shape)\n",
    "    #         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits = tf.matmul(hidden, s_w) + s_b\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "\n",
    "    logits_test = []\n",
    "\n",
    "    with tf.variable_scope('logits0') as scope_1:\n",
    "#         scope_1.reuse_variables()\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "        scope_1.reuse_variables()\n",
    "        test = tf.get_variable('l1_b')\n",
    "\n",
    "    with tf.variable_scope('logits1') as scope_2:\n",
    "#         scope_2.reuse_variables()\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits2') as scope_3:\n",
    "#         scope_3.reuse_variables()\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "    with tf.variable_scope('logits3') as scope_4:\n",
    "#         scope_4.reuse_variables()\n",
    "\n",
    "\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits4') as scope_5:\n",
    "#         scope_5.reuse_variables()\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    test_predict_labels = tf.transpose(tf.argmax(test_prediction, 2))\n",
    "    \n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session,\"/Users/pandaczm/PhD_Projects/GitHub/Udacity_Deep_Learning/Final_Project/CNN_multi_different.ckpt\")\n",
    "    print ('Model restored')\n",
    "    print('Initialized')\n",
    "    test_prediction = session.run(test_predict_labels, feed_dict={tf_test_dataset : test_sample})\n",
    "    print(test_prediction)\n",
    "    test_var = test.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.variable_scope('logits0', reuse=True):\n",
    "        test = tf.get_variable('l1_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00574939  0.10999518 -0.29908305 -0.22000167 -0.13949318  0.04275881\n",
      "  0.00160391 -0.29240328 -0.42073289  0.00295903 -0.24200477 -0.00516193\n",
      " -0.37975165  0.0193522   0.03871866  0.08389094]\n"
     ]
    }
   ],
   "source": [
    "print test_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    accur = 0\n",
    "    for index, item in enumerate(predictions):\n",
    "        accur += np.sum(np.argmax(item, 1) == np.argmax(labels[index], 1))\n",
    "        print accur\n",
    "        print item.shape\n",
    "    return (accur / float(len(predictions) * predictions[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "(55000, 11)\n",
      "110000\n",
      "(55000, 11)\n",
      "165000\n",
      "(55000, 11)\n",
      "220000\n",
      "(55000, 11)\n",
      "275000\n",
      "(55000, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(train_labels,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_one_hot(prob):\n",
    "    prob_one_hot= np.zeros(prob.shape[0])\n",
    "    prob_one_hot[np.argmax(prob)] = 1\n",
    "    return prob_one_hot\n",
    "    \n",
    "def accuracy_ref(predict_num, predictions, labels):\n",
    "    max_prob = []\n",
    "    final_prob = []\n",
    "    for i in predictions:\n",
    "        max_prob.append(np.max(i,axis=1))\n",
    "#     print max_prob\n",
    "    for i in range(4):\n",
    "        tmp = np.log(predict_num[:,i])\n",
    "        for j in range(i+1):\n",
    "#             print np.log(tmp)\n",
    "#             print np.log(max_prob[j])\n",
    "            tmp = tmp + np.log(max_prob[j])\n",
    "#             tmp = tmp * max_prob[j]\n",
    "        final_prob.append(tmp.tolist())\n",
    "    final_prob = np.array(final_prob).T\n",
    "#     print final_prob\n",
    "    final_index = np.argmax(final_prob,axis = 1)\n",
    "#     print final_index\n",
    "    predict_labels = []\n",
    "    for probs in predictions:\n",
    "        predict_labels.append(np.argmax(probs, 1))\n",
    "    predict_labels = np.array(predict_labels).T\n",
    "#     print predict_labels\n",
    "    for index, item in enumerate(final_index):\n",
    "        predict_labels[index , item + 1 :] = 11\n",
    "#     print predict_labels\n",
    "    \n",
    "    return (100.0 * np.sum(predict_labels == labels)) / (len(predictions) * predictions[0].shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_channels = 1\n",
    "def reformat(dataset):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size * 5, num_channels)).astype(np.float32)\n",
    "  return dataset\n",
    "train_dataset_ref = reformat(train_dataset_ref)\n",
    "valid_dataset_ref = reformat(valid_dataset_ref)\n",
    "test_dataset_ref = reformat(test_dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack:0\", shape=(5, 16, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# put the number of the sequence into train.\n",
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "max_seq = 5\n",
    "num_labels_ref = 10\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size * 5, num_channels))\n",
    "    tf_train_labels = list()\n",
    "#     tf_train_num = tf.placeholder(tf.float32, shape = (batch_size))\n",
    "    tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, max_seq)))\n",
    "    for _ in range(max_length):\n",
    "        tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, num_labels_ref)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset_ref)\n",
    "    tf_test_dataset = tf.constant(test_dataset_ref)\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_variable([depth])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_variable([depth])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_variable([num_hidden])\n",
    "    s1_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s1_b = bias_variable([num_labels_ref])\n",
    "    s2_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s2_b = bias_variable([num_labels_ref])\n",
    "    s3_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s3_b = bias_variable([num_labels_ref])\n",
    "    s4_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s4_b = bias_variable([num_labels_ref])\n",
    "    s5_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s5_b = bias_variable([num_labels_ref])\n",
    "    s_w = weight_variable([num_hidden, max_seq])\n",
    "    s_b = bias_variable([max_seq])\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits0 = tf.matmul(hidden,s_w) + s_b\n",
    "        logits1 = tf.matmul(hidden, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(hidden, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(hidden, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(hidden, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(hidden, s5_w) + s5_b\n",
    "\n",
    "        return logits0,logits1,logits2,logits3,logits4,logits5\n",
    "                \n",
    "    \n",
    "        \n",
    "    \n",
    "    # Training computation.\n",
    "    logits= model(tf_train_dataset)\n",
    "    logits_train = model(tf_train_dataset,False)\n",
    "    logits_valid = model(tf_valid_dataset, False)\n",
    "    logits_test = model(tf_test_dataset, False)\n",
    "    loss = 0\n",
    "    iter_num = 0\n",
    "    for i in range(batch_size):\n",
    "        loss += tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[0][i],logits=logits[0][i]) \n",
    "        iter_num +=1\n",
    "        for j in range(max_length):\n",
    "            def f1(): return tf.zeros((1))\n",
    "            def f2(): return tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[j + 1][i],logits=logits[j + 1][i])\n",
    "            def f2_1(): return tf.ones((1))\n",
    "            loss += tf.cond(tf.equal(tf.reduce_sum(tf_train_labels[j + 1][i]),0),f1,f2)\n",
    "            iter_num += tf.cond(tf.equal(tf.reduce_sum(tf_train_labels[j + 1][i]),0),f1,f2_1)\n",
    "    loss = loss/iter_num\n",
    "        \n",
    "#         loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i+1], logits=logits[i+1])) \n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_num = tf.nn.softmax(logits_train[0])\n",
    "    valid_num = tf.nn.softmax(logits_valid[0])\n",
    "    test_num = tf.nn.softmax(logits_test[0])\n",
    "   \n",
    "\n",
    "    train_prediction = tf.stack(( tf.nn.softmax(logits_train[1]), tf.nn.softmax(logits_train[2]), \\\n",
    "                                 tf.nn.softmax(logits_train[3]), tf.nn.softmax(logits_train[4]), tf.nn.softmax(logits_train[5])))\n",
    "    valid_prediction = tf.stack(( tf.nn.softmax(logits_valid[1]), tf.nn.softmax(logits_valid[2]),tf.nn.softmax(logits_valid[3]), tf.nn.softmax(logits_valid[4]), tf.nn.softmax(logits_valid[5])))\n",
    "    test_prediction = tf.stack(( tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4]), tf.nn.softmax(logits_test[5])))\n",
    "    print train_prediction\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.321078\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 41.3%\n",
      "Minibatch loss at step 50: 2.161748\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 100: 2.164496\n",
      "Minibatch accuracy: 36.2%\n",
      "Minibatch loss at step 150: 2.031170\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 200: 2.088078\n",
      "Minibatch accuracy: 45.0%\n",
      "Validation accuracy: 42.2%\n",
      "Minibatch loss at step 250: 1.985770\n",
      "Minibatch accuracy: 46.2%\n",
      "Minibatch loss at step 300: 2.050894\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 350: 1.774233\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 400: 2.004312\n",
      "Minibatch accuracy: 32.5%\n",
      "Validation accuracy: 44.7%\n",
      "Minibatch loss at step 450: 1.950963\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 500: 1.955529\n",
      "Minibatch accuracy: 35.0%\n",
      "Minibatch loss at step 550: 1.658324\n",
      "Minibatch accuracy: 56.2%\n",
      "Minibatch loss at step 600: 1.802962\n",
      "Minibatch accuracy: 53.8%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 650: 1.830178\n",
      "Minibatch accuracy: 43.8%\n",
      "Minibatch loss at step 700: 1.867773\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 750: 1.671474\n",
      "Minibatch accuracy: 48.8%\n",
      "Minibatch loss at step 800: 1.434195\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 59.4%\n",
      "Minibatch loss at step 850: 1.525052\n",
      "Minibatch accuracy: 67.5%\n",
      "Minibatch loss at step 900: 1.674669\n",
      "Minibatch accuracy: 65.0%\n",
      "Minibatch loss at step 950: 1.432139\n",
      "Minibatch accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 1.344984\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 65.6%\n",
      "Minibatch loss at step 1050: 1.345573\n",
      "Minibatch accuracy: 71.2%\n",
      "Minibatch loss at step 1100: 1.602865\n",
      "Minibatch accuracy: 71.2%\n",
      "Minibatch loss at step 1150: 1.478044\n",
      "Minibatch accuracy: 72.5%\n",
      "Minibatch loss at step 1200: 1.376759\n",
      "Minibatch accuracy: 66.2%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 1250: 1.494024\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 1300: 1.316517\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 1350: 1.303177\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 1400: 1.329934\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 1450: 1.295535\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 1500: 1.326293\n",
      "Minibatch accuracy: 71.2%\n",
      "Minibatch loss at step 1550: 1.488133\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 1600: 1.357583\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 1650: 1.453834\n",
      "Minibatch accuracy: 63.8%\n",
      "Minibatch loss at step 1700: 1.375072\n",
      "Minibatch accuracy: 71.2%\n",
      "Minibatch loss at step 1750: 1.101427\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 1800: 1.584789\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 1850: 1.489419\n",
      "Minibatch accuracy: 70.0%\n",
      "Minibatch loss at step 1900: 1.311391\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1950: 1.226908\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 2000: 1.201384\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 2050: 1.437077\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 2100: 1.308108\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 2150: 1.398632\n",
      "Minibatch accuracy: 62.5%\n",
      "Minibatch loss at step 2200: 1.084811\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2250: 1.286070\n",
      "Minibatch accuracy: 73.8%\n",
      "Minibatch loss at step 2300: 1.231144\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 2350: 1.372632\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2400: 1.004073\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2450: 1.381119\n",
      "Minibatch accuracy: 66.2%\n",
      "Minibatch loss at step 2500: 1.146690\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 2550: 0.972173\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 2600: 1.185027\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2650: 1.227709\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 2700: 1.257362\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 2750: 1.087274\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 2800: 1.329530\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2850: 1.157979\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 2900: 1.189427\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 2950: 0.889584\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 3000: 1.092164\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 3050: 1.052368\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 3100: 1.175203\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 3150: 1.025682\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 3200: 0.962631\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 3250: 0.989059\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 3300: 1.075690\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 3350: 1.121220\n",
      "Minibatch accuracy: 82.5%\n",
      "Minibatch loss at step 3400: 1.060199\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3450: 0.952449\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 3500: 1.293737\n",
      "Minibatch accuracy: 76.2%\n",
      "Minibatch loss at step 3550: 1.180100\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 3600: 1.240110\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 3650: 1.346036\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 3700: 0.943528\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 3750: 1.354184\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 3800: 0.774859\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 3850: 1.160856\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 3900: 0.883094\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 3950: 1.124221\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 4000: 1.007515\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 4050: 0.971222\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 4100: 1.078862\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 4150: 1.031786\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 4200: 1.191898\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 4250: 1.188251\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 4300: 0.782505\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 4350: 1.114060\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 4400: 1.100291\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4450: 0.903065\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 4500: 1.023769\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 4550: 1.170976\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 4600: 1.113016\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4650: 0.739646\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 4700: 0.964875\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 4750: 0.806540\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4800: 0.965363\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 4850: 0.944386\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 4900: 1.264593\n",
      "Minibatch accuracy: 82.5%\n",
      "Minibatch loss at step 4950: 1.139569\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 5000: 1.071726\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5050: 0.931813\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 5100: 0.783244\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 5150: 0.793109\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 5200: 1.038317\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 5250: 0.981602\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 5300: 0.975236\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 5350: 1.198623\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 5400: 0.947421\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5450: 1.082891\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 5500: 0.899841\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 5550: 0.918072\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 5600: 1.116428\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 5650: 0.714609\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 5700: 0.919966\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 5750: 0.822864\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 5800: 1.065751\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 5850: 1.004674\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 5900: 0.784232\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 5950: 0.963052\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 6000: 0.716024\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6050: 0.854151\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 6100: 0.874077\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 6150: 0.894737\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 6200: 1.004809\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6250: 0.921341\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 6300: 1.068430\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 6350: 0.858202\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 6400: 1.070364\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6450: 1.020661\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 6500: 0.758274\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 6550: 0.946436\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 6600: 1.066978\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 6650: 0.810650\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 6700: 0.868592\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 6750: 0.844158\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 6800: 1.043426\n",
      "Minibatch accuracy: 82.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 6850: 0.725462\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 6900: 0.824409\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 6950: 0.939888\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 7000: 0.927014\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7050: 1.012997\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 7100: 0.654249\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 7150: 0.765320\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 7200: 1.116445\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 7250: 0.982382\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 7300: 0.913222\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 7350: 0.823218\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 7400: 0.787649\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 7450: 0.772654\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 7500: 0.900327\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 7550: 0.728348\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 7600: 0.733102\n",
      "Minibatch accuracy: 88.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7650: 0.928239\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 7700: 0.892301\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 7750: 0.730819\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 7800: 0.750567\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7850: 0.972907\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 7900: 0.870725\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 7950: 0.818005\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 8000: 0.820398\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8050: 0.901776\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 8100: 0.725423\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 8150: 0.989563\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 8200: 1.041558\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 8250: 0.884306\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 8300: 0.878623\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 8350: 1.062536\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 8400: 0.802788\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 8450: 0.696850\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 8500: 0.751144\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 8550: 0.833673\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 8600: 0.650874\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 8650: 0.929711\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 8700: 0.770269\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 8750: 0.744735\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 8800: 0.936762\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 8850: 1.177714\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 8900: 0.802105\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 8950: 0.812876\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 9000: 0.538739\n",
      "Minibatch accuracy: 97.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 9050: 0.823793\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 9100: 0.947390\n",
      "Minibatch accuracy: 82.5%\n",
      "Minibatch loss at step 9150: 0.720288\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 9200: 0.775952\n",
      "Minibatch accuracy: 92.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 9250: 0.758347\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 9300: 0.848161\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 9350: 0.403077\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 9400: 0.654750\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 9450: 1.032262\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 9500: 1.049858\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 9550: 0.621562\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 9600: 0.956070\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 9650: 0.735806\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 9700: 0.947597\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 9750: 0.772940\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 9800: 0.935579\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 9850: 0.912059\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 9900: 0.786904\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 9950: 0.587548\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 10000: 0.865130\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.5%\n",
      "Test accuracy: 90.6%\n",
      "Model saved in file: CNN_multi_ref.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_dataset_ref.shape[0] - batch_size)\n",
    "        feed_dict = dict()\n",
    "        batch_data = train_dataset_ref[offset:(offset + batch_size), :, :, :]\n",
    "        feed_dict[tf_train_dataset] = batch_data\n",
    "        batch_labels = list()\n",
    "        for i in range(max_length + 1):\n",
    "            batch_labels.append(train_labels_ref[i][offset:(offset + batch_size), :])\n",
    "    \n",
    "            feed_dict[tf_train_labels[i]] = train_labels_ref[i][offset:(offset + batch_size), :]\n",
    "\n",
    "        _, l, predictions, train_n = session.run([optimizer, loss, train_prediction,train_num], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "#             print sum(t_n[offset:(offset + batch_size)])\n",
    "#             print iter_eval\n",
    "#             print layer1_weights.eval()\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy_ref(train_n,predictions, t_l[offset:(offset + batch_size)]))\n",
    "            if (step % 200 == 0):\n",
    "                print('Validation accuracy: %.1f%%' % accuracy_ref(valid_num.eval(), valid_prediction.eval(), v_l))\n",
    "    print('Test accuracy: %.1f%%' % accuracy_ref(test_num.eval(),test_prediction.eval(), te_l))\n",
    "    save_path = saver.save(session, \"CNN_multi_ref.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10)\n"
     ]
    }
   ],
   "source": [
    "print train_labels_ref[1][offset:(offset + batch_size)].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_one_hot(prob):\n",
    "    prob_one_hot= np.zeros(prob.shape[0])\n",
    "    prob_one_hot[np.argmax(prob)] = 1\n",
    "    return prob_one_hot\n",
    "    \n",
    "def accuracy_ref(predict_num, predictions, labels):\n",
    "    max_prob = []\n",
    "    final_prob = []\n",
    "    for i in predictions:\n",
    "        max_prob.append(np.max(i,axis=1))\n",
    "#     print max_prob\n",
    "    for i in range(4):\n",
    "        tmp = np.log(predict_num[:,i])\n",
    "        for j in range(i+1):\n",
    "#             print np.log(tmp)\n",
    "#             print np.log(max_prob[j])\n",
    "            tmp = tmp + np.log(max_prob[j])\n",
    "#             tmp = tmp * max_prob[j]\n",
    "        final_prob.append(tmp.tolist())\n",
    "    final_prob = np.array(final_prob).T\n",
    "#     print final_prob\n",
    "    final_index = np.argmax(final_prob,axis = 1)\n",
    "    print final_index\n",
    "    predict_labels = []\n",
    "    for probs in predictions:\n",
    "        predict_labels.append(np.argmax(probs, 1))\n",
    "    predict_labels = np.array(predict_labels).T\n",
    "    print predict_labels\n",
    "    for index, item in enumerate(final_index):\n",
    "        predict_labels[index , item + 1 :] = 11\n",
    "    print predict_labels\n",
    "    \n",
    "    return (100.0 * np.sum(predict_labels == labels)) / (len(predictions) * predictions[0].shape[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "def accuracy(predictions, labels):\n",
    "    accur = 0\n",
    "    for index, item in enumerate(predictions):\n",
    "        accur += np.sum(np.argmax(item, 1) == np.argmax(labels[index], 1))\n",
    "#         print accur\n",
    "#         print item.shape\n",
    "    return (100.0 * accur / float(len(predictions) * predictions[0].shape[0]))\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[0.4,0.1,0.3,0.1,0.1],[0.2,0.9,0.1,0.1,0.1]])\n",
    "b = np.array([[0.1,0.2,0.3],[0.10,0.2,0.1]])\n",
    "c = np.array([[0.2,0.5,0.2],[0.20,0.1,0.2]])\n",
    "d = np.array([[2,1,4],[9,1,7]])\n",
    "e = np.array([[4,2,1],[0.1,0.4,0.1]])\n",
    "test = []\n",
    "test.append(b)\n",
    "test.append(c)\n",
    "test.append(d)\n",
    "test.append(e)\n",
    "test = np.array(test)\n",
    "label = np.array([[2,1,2,0],[1,11,11,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0]\n",
      "[[2 1 2 0]\n",
      " [1 0 0 1]]\n",
      "[[ 2  1  2  0]\n",
      " [ 1 11 11 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ref(a,test,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[0] = prob_one_hot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [3, 2, 1]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_1 = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_1[2:,1,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [3, 2, 1]],\n",
       "\n",
       "       [[3, 2, 2],\n",
       "        [5, 4, 2]],\n",
       "\n",
       "       [[2, 1, 4],\n",
       "        [0, 0, 0]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis(=1) out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-fa8da72b18b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axis(=1) out of bounds"
     ]
    }
   ],
   "source": [
    "np.argmax([0,0,0],axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
