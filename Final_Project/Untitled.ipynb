{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from six.moves import cPickle as pickle\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x110f10090>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = mnist.train.images[7]\n",
    "b = mnist.train.images[2]\n",
    "a = a.reshape(28,28)\n",
    "b = b.reshape(28,28)\n",
    "c = np.zeros((28,28))\n",
    "d = np.concatenate((a,b,c),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784) (10000, 784) (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "print mnist.train.images.shape,mnist.test.images.shape,mnist.validation.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAADGCAYAAAA3xDJsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11519c890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(d)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Generate_dataset(original_dataset, dataset_size, max_length):\n",
    "    labels_list = list()\n",
    "    data_list = list()\n",
    "    original_size = original_dataset.images.shape[0]\n",
    "    for i in range(max_length):\n",
    "        labels_list.append([])\n",
    "    for i in range(dataset_size):\n",
    "        num_length = random.randint(1, max_length)\n",
    "        tmp = list()\n",
    "        for j in range(max_length):\n",
    "            if j < num_length:\n",
    "                index = random.randint(0, original_size - 1)\n",
    "#                 print index\n",
    "                tmp.append(original_dataset.images[index].reshape(28,28))\n",
    "                data_label = np.concatenate((original_dataset.labels[index], np.array([0])), axis = 0)\n",
    "                labels_list[j].append(data_label.tolist())\n",
    "                \n",
    "            else:\n",
    "                tmp.append(np.zeros((28,28)))\n",
    "                data_label = np.concatenate((np.zeros(10), np.array([1])), axis=0)\n",
    "                labels_list[j].append(data_label.tolist())\n",
    "        data_list.append(np.concatenate(tmp, axis = 1).reshape((1, 28 * 28 * max_length)))\n",
    "    labels_list = [np.array(i) for i in labels_list]\n",
    "        \n",
    "    return np.concatenate(data_list, axis = 0), labels_list\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(num_length):\n",
    "    label_num = [0,0,0,0,0]\n",
    "    label_num[num_length - 1] = 1\n",
    "    return label_num\n",
    "\n",
    "def Generate_dataset_1(original_dataset, dataset_size, max_length):\n",
    "    labels_list = list()\n",
    "    data_list = list()\n",
    "    labels = list()\n",
    "    original_size = original_dataset.images.shape[0]\n",
    "    for i in range(max_length + 1):\n",
    "        labels_list.append([])\n",
    "    for i in range(dataset_size):\n",
    "        num_length = random.randint(1, max_length)\n",
    "        labels_list[0].append(one_hot(num_length))\n",
    "        tmp = list()\n",
    "        label_tmp = []\n",
    "        for j in range(max_length):\n",
    "            if j < num_length:\n",
    "                index = random.randint(0, original_size - 1)\n",
    "#                 print index\n",
    "                tmp.append(original_dataset.images[index].reshape(28,28))\n",
    "                data_label = original_dataset.labels[index]\n",
    "                labels_list[j + 1].append(data_label.tolist())\n",
    "                label_tmp.append(np.argmax(data_label).tolist())\n",
    "                \n",
    "            else:\n",
    "                tmp.append(np.zeros((28,28)))\n",
    "                data_label = np.zeros(10)\n",
    "                labels_list[j + 1].append(data_label.tolist())\n",
    "                label_tmp.append(11)\n",
    "        data_list.append(np.concatenate(tmp, axis = 1).reshape((1, 28 * 28 * max_length)))\n",
    "        labels.append(label_tmp)\n",
    "    labels_list = [np.array(i) for i in labels_list]\n",
    "        \n",
    "    return np.concatenate(data_list, axis = 0), labels_list, np.array(labels)\n",
    "        \n",
    "                \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset, train_labels = Generate_dataset(mnist.train,55000,5)\n",
    "valid_dataset, valid_labels = Generate_dataset(mnist.validation,5000,5)\n",
    "test_dataset, test_labels = Generate_dataset(mnist.test,10000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset_ref, train_labels_ref, t_l = Generate_dataset_1(mnist.train,55000,5)\n",
    "valid_dataset_ref, valid_labels_ref,v_l = Generate_dataset_1(mnist.validation,5000,5)\n",
    "test_dataset_ref, test_labels_ref,te_l = Generate_dataset_1(mnist.test,10000,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test =train_dataset_ref[1].reshape(28,140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x187609150>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAACJCAYAAADJyorVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x180b9d9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_ref[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'sequence_ref.pickle'\n",
    "try:\n",
    "  with open(pickle_file, 'wb') as f:\n",
    "      save = {\n",
    "        'train_dataset_ref': train_dataset_ref,\n",
    "        'train_labels_ref': train_labels_ref,\n",
    "        'valid_dataset_ref': valid_dataset_ref,\n",
    "        'valid_labels_ref': valid_labels_ref,\n",
    "        'test_dataset_ref': test_dataset_ref,\n",
    "        'test_labels_ref': test_labels_ref,\n",
    "        }\n",
    "      pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle_file = 'sequence.pickle'\n",
    "try:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        dataset_dict = pickle.load(f)\n",
    "        train_dataset = dataset_dict['train_dataset']\n",
    "        test_dataset = dataset_dict['test_dataset']\n",
    "        train_labels = dataset_dict['train_labels']\n",
    "        valid_dataset = dataset_dict['valid_dataset']\n",
    "        valid_labels = dataset_dict['valid_labels']\n",
    "        test_labels = dataset_dict['test_labels']\n",
    "    \n",
    "except Exception as e:\n",
    "    print ('Unable to load the data')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (55000, 28, 140, 1))\n",
      "('Validation set', (5000, 28, 140, 1))\n",
      "('Test set', (10000, 28, 140, 1))\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size * 5, num_channels)).astype(np.float32)\n",
    "  return dataset\n",
    "train_dataset = reformat(train_dataset, train_labels)\n",
    "valid_dataset = reformat(valid_dataset, valid_labels)\n",
    "test_dataset = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape)\n",
    "print('Validation set', valid_dataset.shape)\n",
    "print('Test set', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11)\n"
     ]
    }
   ],
   "source": [
    "test = test_dataset[100]\n",
    "print test_labels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "max_length = 5\n",
    "num_labels = 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "def accuracy(predictions, labels):\n",
    "    accur = 0\n",
    "    for index, item in enumerate(predictions):\n",
    "        accur += np.sum(np.argmax(item, 1) == np.argmax(labels[index], 1))\n",
    "#         print accur\n",
    "#         print item.shape\n",
    "    return (100.0 * accur / float(len(predictions) * predictions[0].shape[0]))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size * 5, num_channels))\n",
    "    tf_train_labels = list()\n",
    "    for _ in range(max_length):\n",
    "        tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, num_labels)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_variable([depth])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_variable([depth])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_variable([num_hidden])\n",
    "    s1_w = weight_variable([num_hidden,num_labels])\n",
    "    s1_b = bias_variable([num_labels])\n",
    "    s2_w = weight_variable([num_hidden,num_labels])\n",
    "    s2_b = bias_variable([num_labels])\n",
    "    s3_w = weight_variable([num_hidden,num_labels])\n",
    "    s3_b = bias_variable([num_labels])\n",
    "    s4_w = weight_variable([num_hidden,num_labels])\n",
    "    s4_b = bias_variable([num_labels])\n",
    "    s5_w = weight_variable([num_hidden,num_labels])\n",
    "    s5_b = bias_variable([num_labels])\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits1 = tf.matmul(hidden, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(hidden, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(hidden, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(hidden, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(hidden, s5_w) + s5_b\n",
    "\n",
    "        return [logits1,logits2,logits3,logits4,logits5]\n",
    "    \n",
    "    # Training computation.\n",
    "    logits= model(tf_train_dataset)\n",
    "    logits_train = model(tf_train_dataset,False)\n",
    "    logits_valid = model(tf_valid_dataset, False)\n",
    "    logits_test = model(tf_test_dataset, False)\n",
    "    loss = 0\n",
    "    for i in range(max_length):\n",
    "        loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i], logits=logits[i])) \n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "\n",
    "    train_prediction = tf.stack((tf.nn.softmax(logits_train[0]), tf.nn.softmax(logits_train[1]), tf.nn.softmax(logits_train[2]), \\\n",
    "                                 tf.nn.softmax(logits_train[3]), tf.nn.softmax(logits_train[4])))\n",
    "    valid_prediction = tf.stack((tf.nn.softmax(logits_valid[0]), tf.nn.softmax(logits_valid[1]), tf.nn.softmax(logits_valid[2]),tf.nn.softmax(logits_valid[3]), tf.nn.softmax(logits_valid[4])))\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    \n",
    "#     saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 23.320377\n",
      "Minibatch accuracy: 5.0%\n",
      "Validation accuracy: 28.2%\n",
      "Minibatch loss at step 50: 7.732985\n",
      "Minibatch accuracy: 46.2%\n",
      "Minibatch loss at step 100: 7.462679\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 48.2%\n",
      "Minibatch loss at step 150: 7.145736\n",
      "Minibatch accuracy: 60.0%\n",
      "Minibatch loss at step 200: 9.342194\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 54.9%\n",
      "Minibatch loss at step 250: 7.287964\n",
      "Minibatch accuracy: 58.8%\n",
      "Minibatch loss at step 300: 7.254318\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 61.5%\n",
      "Minibatch loss at step 350: 5.233470\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 400: 5.606007\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 66.0%\n",
      "Minibatch loss at step 450: 5.237596\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 500: 4.525781\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 550: 5.880080\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 600: 6.721588\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 71.7%\n",
      "Minibatch loss at step 650: 6.681174\n",
      "Minibatch accuracy: 65.0%\n",
      "Minibatch loss at step 700: 6.314043\n",
      "Minibatch accuracy: 73.8%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 750: 6.356209\n",
      "Minibatch accuracy: 68.8%\n",
      "Minibatch loss at step 800: 6.111434\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 850: 6.387175\n",
      "Minibatch accuracy: 72.5%\n",
      "Minibatch loss at step 900: 6.368301\n",
      "Minibatch accuracy: 66.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 950: 6.129025\n",
      "Minibatch accuracy: 73.8%\n",
      "Minibatch loss at step 1000: 5.405671\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.2%\n",
      "Test accuracy: 78.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "    feed_dict = dict()\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    feed_dict[tf_train_dataset] = batch_data\n",
    "    batch_labels = list()\n",
    "    for i in range(max_length):\n",
    "        batch_labels.append(train_labels[i][offset:(offset + batch_size), :])\n",
    "        feed_dict[tf_train_labels[i]] = train_labels[i][offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "    if (step % 100 == 0):\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "#   save_path = saver.save(session, \"CNN_multi.ckpt\")\n",
    "#   print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 140, 1)\n"
     ]
    }
   ],
   "source": [
    "print test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_test = []\n",
    "for i in range(5):\n",
    "    sample_test_index = random.randint(0,9999)\n",
    "    sample_test.append(sample_test_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sample = test_dataset[sample_test,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (5, image_size, image_size * 5, num_channels))\n",
    " \n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_variable([depth])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_variable([depth])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_variable([num_hidden])\n",
    "    s1_w = weight_variable([num_hidden,num_labels])\n",
    "    s1_b = bias_variable([num_labels])\n",
    "    s2_w = weight_variable([num_hidden,num_labels])\n",
    "    s2_b = bias_variable([num_labels])\n",
    "    s3_w = weight_variable([num_hidden,num_labels])\n",
    "    s3_b = bias_variable([num_labels])\n",
    "    s4_w = weight_variable([num_hidden,num_labels])\n",
    "    s4_b = bias_variable([num_labels])\n",
    "    s5_w = weight_variable([num_hidden,num_labels])\n",
    "    s5_b = bias_variable([num_labels])\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits1 = tf.matmul(hidden, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(hidden, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(hidden, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(hidden, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(hidden, s5_w) + s5_b\n",
    "\n",
    "        return [logits1,logits2,logits3,logits4,logits5]\n",
    "    \n",
    "    # Training computation.\n",
    "    logits= model(tf_test_dataset)\n",
    "   \n",
    "    # Predictions for the training, validation, and test data.\n",
    "\n",
    "    logits_test = model(tf_test_dataset, False)\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    test_predict_labels = tf.transpose(tf.argmax(test_prediction, 2))\n",
    "\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Initialized\n",
      "[[ 7  3 10 10 10]\n",
      " [ 7  8 10 10 10]\n",
      " [ 9  4  3  9 10]\n",
      " [ 9  9  1  0  3]\n",
      " [ 0  6 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"/Users/pandaczm/PhD_Projects/GitHub/Udacity_Deep_Learning/Final_Project/CNN_multi.ckpt\")\n",
    "  print(\"Model restored.\")  \n",
    "\n",
    "  print('Initialized')\n",
    "  test_prediction = session.run(test_predict_labels, feed_dict={tf_test_dataset : test_sample})\n",
    "  print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_get_variable(name, shape):\n",
    "    weight = tf.get_variable(name,shape,initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    return weight\n",
    "\n",
    "def bias_get_variable(name,shape):\n",
    "    bias = tf.get_variable(name,shape,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits0/l1_b:0\n",
      "logits0/l1_b:0\n",
      "logits0/l1_b:0\n",
      "logits0/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits4/l1_b:0\n",
      "logits4/l1_b:0\n",
      "logits4/l1_b:0\n",
      "logits4/l1_b:0\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size * 5, num_channels))\n",
    "    tf_train_labels = list()\n",
    "    for _ in range(max_length):\n",
    "        tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, num_labels)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset[:2000])\n",
    "    \n",
    "    # Variables.\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        layer1_weights = weight_get_variable('l1_w',[patch_size, patch_size, num_channels, depth])\n",
    "        layer1_bias = bias_get_variable('l1_b',[depth])\n",
    "        layer2_weights = weight_get_variable('l2_w',[patch_size, patch_size, depth, depth])\n",
    "        layer2_bias = bias_get_variable('l2_b',[depth])\n",
    "        layer3_weights = weight_get_variable('l3_w',[image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "        layer3_bias = bias_get_variable('l3_b',[num_hidden])\n",
    "        s_w = weight_get_variable('f_w', [num_hidden,num_labels])\n",
    "        s_b = bias_get_variable('f_b',[num_labels])\n",
    "\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        print layer1_bias.name\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits = tf.matmul(hidden, s_w) + s_b\n",
    "\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = []\n",
    "    logits_train = []\n",
    "    logits_test = []\n",
    "    logits_valid = []\n",
    "    with tf.variable_scope('logits0') as scope_1:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_1.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits1') as scope_2:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_2.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits2') as scope_3:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_3.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits3') as scope_4:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_4.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits4') as scope_5:\n",
    "        \n",
    "        logits.append(model(tf_train_dataset))\n",
    "        scope_5.reuse_variables()\n",
    "        logits_train.append(model(tf_train_dataset, False))\n",
    "        logits_valid.append(model(tf_valid_dataset, False))\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    loss = 0\n",
    "    for i in range(max_length):\n",
    "        loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i], logits=logits[i])) \n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "\n",
    "    train_prediction = tf.stack((tf.nn.softmax(logits_train[0]), tf.nn.softmax(logits_train[1]), tf.nn.softmax(logits_train[2]), \\\n",
    "                                 tf.nn.softmax(logits_train[3]), tf.nn.softmax(logits_train[4])))\n",
    "    valid_prediction = tf.stack((tf.nn.softmax(logits_valid[0]), tf.nn.softmax(logits_valid[1]), tf.nn.softmax(logits_valid[2]),tf.nn.softmax(logits_valid[3]), tf.nn.softmax(logits_valid[4])))\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    \n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.265629\n",
      "Minibatch accuracy: 3.8%\n",
      "Validation accuracy: 33.1%\n",
      "Minibatch loss at step 50: 7.827024\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 100: 7.709940\n",
      "Minibatch accuracy: 41.2%\n",
      "Minibatch loss at step 150: 7.059905\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 200: 9.057103\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 250: 7.078635\n",
      "Minibatch accuracy: 58.8%\n",
      "Minibatch loss at step 300: 6.898920\n",
      "Minibatch accuracy: 60.0%\n",
      "Minibatch loss at step 350: 5.031794\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 400: 5.242235\n",
      "Minibatch accuracy: 77.5%\n",
      "Minibatch loss at step 450: 4.755205\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 500: 3.641416\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 75.4%\n",
      "Minibatch loss at step 550: 4.422575\n",
      "Minibatch accuracy: 83.8%\n",
      "Minibatch loss at step 600: 5.102674\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 650: 4.711769\n",
      "Minibatch accuracy: 76.2%\n",
      "Minibatch loss at step 700: 4.316792\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 750: 4.756740\n",
      "Minibatch accuracy: 73.8%\n",
      "Minibatch loss at step 800: 4.178140\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 850: 5.304820\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 900: 4.732872\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 950: 3.924833\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1000: 3.861768\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1050: 3.389151\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 1100: 4.181420\n",
      "Minibatch accuracy: 82.5%\n",
      "Minibatch loss at step 1150: 3.639707\n",
      "Minibatch accuracy: 78.8%\n",
      "Minibatch loss at step 1200: 3.955760\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1250: 3.872199\n",
      "Minibatch accuracy: 81.2%\n",
      "Minibatch loss at step 1300: 4.280478\n",
      "Minibatch accuracy: 85.0%\n",
      "Minibatch loss at step 1350: 4.299892\n",
      "Minibatch accuracy: 80.0%\n",
      "Minibatch loss at step 1400: 2.398077\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 1450: 3.723983\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 1500: 3.747082\n",
      "Minibatch accuracy: 86.2%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 1550: 2.093361\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 1600: 3.178032\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 1650: 4.118211\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 1700: 3.693977\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 1750: 2.481850\n",
      "Minibatch accuracy: 87.5%\n",
      "Minibatch loss at step 1800: 2.572172\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 1850: 3.004564\n",
      "Minibatch accuracy: 88.8%\n",
      "Minibatch loss at step 1900: 2.333255\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 1950: 3.566700\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2000: 2.235529\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 91.9%\n",
      "Minibatch loss at step 2050: 2.061288\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2100: 2.770049\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 2150: 3.114858\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2200: 2.977314\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 2250: 3.831862\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2300: 2.830133\n",
      "Minibatch accuracy: 90.0%\n",
      "Minibatch loss at step 2350: 2.688179\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 2400: 2.345165\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 2450: 2.267612\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 2500: 2.098443\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 2550: 2.405329\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2600: 2.409330\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 2650: 1.622021\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2700: 2.937958\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 2750: 2.057053\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 2800: 2.603288\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 2850: 3.022257\n",
      "Minibatch accuracy: 86.2%\n",
      "Minibatch loss at step 2900: 3.043956\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 2950: 2.579618\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 3000: 1.959378\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 94.6%\n",
      "Minibatch loss at step 3050: 1.254169\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 3100: 1.304604\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3150: 1.609709\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3200: 2.052211\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 3250: 2.796769\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3300: 2.424225\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 3350: 1.528197\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3400: 1.566339\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 3450: 1.215003\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 3500: 2.037157\n",
      "Minibatch accuracy: 95.0%\n",
      "Validation accuracy: 95.1%\n",
      "Minibatch loss at step 3550: 2.086021\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3600: 1.721193\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3650: 1.837015\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3700: 1.694475\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 3750: 1.211546\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 3800: 1.561422\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 3850: 2.095030\n",
      "Minibatch accuracy: 93.8%\n",
      "Minibatch loss at step 3900: 1.043255\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 3950: 1.159370\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4000: 1.826863\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 95.5%\n",
      "Minibatch loss at step 4050: 1.400965\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4100: 2.203825\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4150: 1.633132\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4200: 1.611597\n",
      "Minibatch accuracy: 95.0%\n",
      "Minibatch loss at step 4250: 1.903408\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4300: 1.274378\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4350: 2.382059\n",
      "Minibatch accuracy: 92.5%\n",
      "Minibatch loss at step 4400: 1.517017\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4450: 1.407132\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4500: 1.586271\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 96.1%\n",
      "Minibatch loss at step 4550: 1.325540\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4600: 2.023803\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4650: 1.659499\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 4700: 1.271218\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4750: 2.150717\n",
      "Minibatch accuracy: 91.2%\n",
      "Minibatch loss at step 4800: 1.673336\n",
      "Minibatch accuracy: 96.2%\n",
      "Minibatch loss at step 4850: 1.015289\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4900: 1.518059\n",
      "Minibatch accuracy: 97.5%\n",
      "Minibatch loss at step 4950: 0.931153\n",
      "Minibatch accuracy: 98.8%\n",
      "Minibatch loss at step 5000: 1.380117\n",
      "Minibatch accuracy: 96.2%\n",
      "Validation accuracy: 96.4%\n",
      "Test accuracy: 96.8%\n",
      "Model saved in file: CNN_multi_different.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "    feed_dict = dict()\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    feed_dict[tf_train_dataset] = batch_data\n",
    "    batch_labels = list()\n",
    "    for i in range(max_length):\n",
    "        batch_labels.append(train_labels[i][offset:(offset + batch_size), :])\n",
    "        feed_dict[tf_train_labels[i]] = train_labels[i][offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "    if (step % 500 == 0):\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), [test_labels[0][:2000],test_labels[1][:2000], test_labels[2][:2000], test_labels[3][:2000], test_labels[4][:2000]]))\n",
    "  save_path = saver.save(session, \"CNN_multi_different.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits0/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits4/l1_b:0\n",
      "Model restored\n",
      "Initialized\n",
      "[[ 3  1  5  7 10]\n",
      " [ 2  5  9  2  7]\n",
      " [ 9 10 10 10 10]\n",
      " [ 3  8  6  9  6]\n",
      " [ 8 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "\n",
    "tf_test_dataset = tf.placeholder(tf.float32, shape = (5, image_size, image_size * 5, num_channels))\n",
    "\n",
    "# Variables.\n",
    "def model(data,train_only = True):\n",
    "    layer1_weights = weight_get_variable('l1_w',[patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_get_variable('l1_b',[depth])\n",
    "    layer2_weights = weight_get_variable('l2_w',[patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_get_variable('l2_b',[depth])\n",
    "    layer3_weights = weight_get_variable('l3_w',[image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_get_variable('l3_b',[num_hidden])\n",
    "    s_w = weight_get_variable('f_w', [num_hidden,num_labels])\n",
    "    s_b = bias_get_variable('f_b',[num_labels])\n",
    "\n",
    "    conv = conv2d(data, layer1_weights)\n",
    "    hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "    print layer1_bias.name\n",
    "    h_pool_1 = max_pool_2x2(hidden_1)\n",
    "    if train_only:\n",
    "        h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "    conv = conv2d(h_pool_1, layer2_weights)\n",
    "    hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "    h_pool_2 = max_pool_2x2(hidden_2)\n",
    "    if train_only:\n",
    "        h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "    shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "    reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "    if train_only:\n",
    "        hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "    logits = tf.matmul(hidden, s_w) + s_b\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "\n",
    "# Training computation.\n",
    "\n",
    "logits_test = []\n",
    "\n",
    "with tf.variable_scope('logits0') as scope_1:\n",
    "    scope_1.reuse_variables()\n",
    "\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "with tf.variable_scope('logits1') as scope_2:\n",
    "    scope_2.reuse_variables()\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "with tf.variable_scope('logits2') as scope_3:\n",
    "    scope_3.reuse_variables()\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "with tf.variable_scope('logits3') as scope_4:\n",
    "    scope_4.reuse_variables()\n",
    "\n",
    "\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "with tf.variable_scope('logits4') as scope_5:\n",
    "    scope_5.reuse_variables()\n",
    "\n",
    "    logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "\n",
    "test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "test_predict_labels = tf.transpose(tf.argmax(test_prediction, 2))\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "session = tf.Session()\n",
    "saver.restore(session,\"/Users/pandaczm/PhD_Projects/GitHub/Udacity_Deep_Learning/Final_Project/CNN_multi_different.ckpt\")\n",
    "print ('Model restored')\n",
    "print('Initialized')\n",
    "test_prediction = session.run(test_predict_labels, feed_dict={tf_test_dataset : test_sample})\n",
    "print(test_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits0/l1_b:0\n",
      "logits1/l1_b:0\n",
      "logits2/l1_b:0\n",
      "logits3/l1_b:0\n",
      "logits4/l1_b:0\n",
      "Model restored\n",
      "Initialized\n",
      "[[ 3  1  5  7 10]\n",
      " [ 2  5  9  2  7]\n",
      " [ 9 10 10 10 10]\n",
      " [ 3  8  6  9  6]\n",
      " [ 8 10 10 10 10]]\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_test_dataset = tf.placeholder(tf.float32, shape = (5, image_size, image_size * 5, num_channels))\n",
    "\n",
    "    # Variables.\n",
    "    def model(data,train_only = True):\n",
    "        layer1_weights = weight_get_variable('l1_w',[patch_size, patch_size, num_channels, depth])\n",
    "        layer1_bias = bias_get_variable('l1_b',[depth])\n",
    "        layer2_weights = weight_get_variable('l2_w',[patch_size, patch_size, depth, depth])\n",
    "        layer2_bias = bias_get_variable('l2_b',[depth])\n",
    "        layer3_weights = weight_get_variable('l3_w',[image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "        layer3_bias = bias_get_variable('l3_b',[num_hidden])\n",
    "        s_w = weight_get_variable('f_w', [num_hidden,num_labels])\n",
    "        s_b = bias_get_variable('f_b',[num_labels])\n",
    "\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        print layer1_bias.name\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "    #         print type(shape)\n",
    "    #         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits = tf.matmul(hidden, s_w) + s_b\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "    # Training computation.\n",
    "\n",
    "    logits_test = []\n",
    "\n",
    "    with tf.variable_scope('logits0') as scope_1:\n",
    "#         scope_1.reuse_variables()\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "        scope_1.reuse_variables()\n",
    "        test = tf.get_variable('l1_b')\n",
    "\n",
    "    with tf.variable_scope('logits1') as scope_2:\n",
    "#         scope_2.reuse_variables()\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits2') as scope_3:\n",
    "#         scope_3.reuse_variables()\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "    with tf.variable_scope('logits3') as scope_4:\n",
    "#         scope_4.reuse_variables()\n",
    "\n",
    "\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "    with tf.variable_scope('logits4') as scope_5:\n",
    "#         scope_5.reuse_variables()\n",
    "\n",
    "        logits_test.append(model(tf_test_dataset, False))\n",
    "\n",
    "\n",
    "    test_prediction = tf.stack((tf.nn.softmax(logits_test[0]), tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4])))\n",
    "    test_predict_labels = tf.transpose(tf.argmax(test_prediction, 2))\n",
    "    \n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session,\"/Users/pandaczm/PhD_Projects/GitHub/Udacity_Deep_Learning/Final_Project/CNN_multi_different.ckpt\")\n",
    "    print ('Model restored')\n",
    "    print('Initialized')\n",
    "    test_prediction = session.run(test_predict_labels, feed_dict={tf_test_dataset : test_sample})\n",
    "    print(test_prediction)\n",
    "    test_var = test.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.variable_scope('logits0', reuse=True):\n",
    "        test = tf.get_variable('l1_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00574939  0.10999518 -0.29908305 -0.22000167 -0.13949318  0.04275881\n",
      "  0.00160391 -0.29240328 -0.42073289  0.00295903 -0.24200477 -0.00516193\n",
      " -0.37975165  0.0193522   0.03871866  0.08389094]\n"
     ]
    }
   ],
   "source": [
    "print test_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    accur = 0\n",
    "    for index, item in enumerate(predictions):\n",
    "        accur += np.sum(np.argmax(item, 1) == np.argmax(labels[index], 1))\n",
    "        print accur\n",
    "        print item.shape\n",
    "    return (accur / float(len(predictions) * predictions[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "(55000, 11)\n",
      "110000\n",
      "(55000, 11)\n",
      "165000\n",
      "(55000, 11)\n",
      "220000\n",
      "(55000, 11)\n",
      "275000\n",
      "(55000, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(train_labels,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_one_hot(prob):\n",
    "    prob_one_hot= np.zeros(prob.shape[0])\n",
    "    prob_one_hot[np.argmax(prob)] = 1\n",
    "    return prob_one_hot\n",
    "    \n",
    "def accuracy_ref(predict_num, predictions, labels):\n",
    "    max_prob = []\n",
    "    final_prob = []\n",
    "    for i in predictions:\n",
    "        max_prob.append(np.max(i,axis=1))\n",
    "#     print max_prob\n",
    "    for i in range(4):\n",
    "        tmp = np.log(predict_num[:,i])\n",
    "        for j in range(i+1):\n",
    "#             print np.log(tmp)\n",
    "#             print np.log(max_prob[j])\n",
    "            tmp = tmp + np.log(max_prob[j])\n",
    "#             tmp = tmp * max_prob[j]\n",
    "        final_prob.append(tmp.tolist())\n",
    "    final_prob = np.array(final_prob).T\n",
    "#     print final_prob\n",
    "    final_index = np.argmax(final_prob,axis = 1)\n",
    "#     print final_index\n",
    "    predict_labels = []\n",
    "    for probs in predictions:\n",
    "        predict_labels.append(np.argmax(probs, 1))\n",
    "    predict_labels = np.array(predict_labels).T\n",
    "#     print predict_labels\n",
    "    for index, item in enumerate(final_index):\n",
    "        predict_labels[index , item + 1 :] = 11\n",
    "#     print predict_labels\n",
    "    \n",
    "    return (100.0 * np.sum(predict_labels == labels)) / (len(predictions) * predictions[0].shape[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reformat(dataset):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size * 5, num_channels)).astype(np.float32)\n",
    "  return dataset\n",
    "train_dataset_ref = reformat(train_dataset_ref)\n",
    "valid_dataset_ref = reformat(valid_dataset_ref)\n",
    "test_dataset_ref = reformat(test_dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack:0\", shape=(5, 16, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# put the number of the sequence into train.\n",
    "keep_prob = 0.5\n",
    "graph = tf.Graph()\n",
    "max_seq = 5\n",
    "num_labels_ref = 10\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size, image_size * 5, num_channels))\n",
    "    tf_train_labels = list()\n",
    "    tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, max_seq)))\n",
    "    for _ in range(max_length):\n",
    "        tf_train_labels.append(tf.placeholder(tf.float32, shape = (batch_size, num_labels_ref)))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    layer1_weights = weight_variable([patch_size, patch_size, num_channels, depth])\n",
    "    layer1_bias = bias_variable([depth])\n",
    "    layer2_weights = weight_variable([patch_size, patch_size, depth, depth])\n",
    "    layer2_bias = bias_variable([depth])\n",
    "    layer3_weights = weight_variable([image_size // 4 * image_size *5 // 4 * depth, num_hidden])\n",
    "    layer3_bias = bias_variable([num_hidden])\n",
    "    s1_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s1_b = bias_variable([num_labels_ref])\n",
    "    s2_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s2_b = bias_variable([num_labels_ref])\n",
    "    s3_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s3_b = bias_variable([num_labels_ref])\n",
    "    s4_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s4_b = bias_variable([num_labels_ref])\n",
    "    s5_w = weight_variable([num_hidden,num_labels_ref])\n",
    "    s5_b = bias_variable([num_labels_ref])\n",
    "    s_w = weight_variable([num_hidden, max_seq])\n",
    "    s_b = bias_variable([max_seq])\n",
    "    \n",
    "    def model(data,train_only = True):\n",
    "        conv = conv2d(data, layer1_weights)\n",
    "        hidden_1 = tf.nn.relu(conv + layer1_bias)\n",
    "        h_pool_1 = max_pool_2x2(hidden_1)\n",
    "        if train_only:\n",
    "            h_pool_1 = tf.nn.dropout(h_pool_1,keep_prob)\n",
    "        conv = conv2d(h_pool_1, layer2_weights)\n",
    "        hidden_2 = tf.nn.relu(conv + layer2_bias)\n",
    "        h_pool_2 = max_pool_2x2(hidden_2)\n",
    "        if train_only:\n",
    "            h_pool_2 = tf.nn.dropout(h_pool_2,keep_prob)\n",
    "        shape = h_pool_2.get_shape().as_list()\n",
    "#         print type(shape)\n",
    "#         print shape\n",
    "        reshape = tf.reshape(h_pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_bias)\n",
    "        if train_only:\n",
    "            hidden = tf.nn.dropout(hidden,keep_prob)\n",
    "        logits0 = tf.matmul(hidden,s_w) + s_b\n",
    "        logits1 = tf.matmul(hidden, s1_w) + s1_b\n",
    "        logits2 = tf.matmul(hidden, s2_w) + s2_b\n",
    "        logits3 = tf.matmul(hidden, s3_w) + s3_b\n",
    "        logits4 = tf.matmul(hidden, s4_w) + s4_b\n",
    "        logits5 = tf.matmul(hidden, s5_w) + s5_b\n",
    "\n",
    "        return [logits0,logits1,logits2,logits3,logits4,logits5]\n",
    "    \n",
    "    # Training computation.\n",
    "    logits= model(tf_train_dataset)\n",
    "    logits_train = model(tf_train_dataset,False)\n",
    "    logits_valid = model(tf_valid_dataset, False)\n",
    "    logits_test = model(tf_test_dataset, False)\n",
    "    loss = 0\n",
    "    for i in range(max_length + 1):\n",
    "        loss += tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels[i], logits=logits[i])) \n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_num = tf.nn.softmax(logits_train[0])\n",
    "    valid_num = tf.nn.softmax(logits_valid[0])\n",
    "    test_num = tf.nn.softmax(logits_test[0])\n",
    "   \n",
    "\n",
    "    train_prediction = tf.stack(( tf.nn.softmax(logits_train[1]), tf.nn.softmax(logits_train[2]), \\\n",
    "                                 tf.nn.softmax(logits_train[3]), tf.nn.softmax(logits_train[4]), tf.nn.softmax(logits_train[5])))\n",
    "    valid_prediction = tf.stack(( tf.nn.softmax(logits_valid[1]), tf.nn.softmax(logits_valid[2]),tf.nn.softmax(logits_valid[3]), tf.nn.softmax(logits_valid[4]), tf.nn.softmax(logits_valid[5])))\n",
    "    test_prediction = tf.stack(( tf.nn.softmax(logits_test[1]), tf.nn.softmax(logits_test[2]),tf.nn.softmax(logits_test[3]), tf.nn.softmax(logits_test[4]), tf.nn.softmax(logits_test[5])))\n",
    "    print train_prediction\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 15.229631\n",
      "Minibatch accuracy: 45.0%\n",
      "Validation accuracy: 41.8%\n",
      "Minibatch loss at step 2: 8.818192\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 4: 8.421157\n",
      "Minibatch accuracy: 46.2%\n",
      "Minibatch loss at step 6: 7.389465\n",
      "Minibatch accuracy: 51.2%\n",
      "Minibatch loss at step 8: 7.964194\n",
      "Minibatch accuracy: 46.2%\n",
      "Minibatch loss at step 10: 8.592332\n",
      "Minibatch accuracy: 47.5%\n",
      "Minibatch loss at step 12: 11.230513\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 14: 9.199974\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 16: 8.452544\n",
      "Minibatch accuracy: 53.8%\n",
      "Minibatch loss at step 18: 22.286615\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 20: 9.424908\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 22: 22.382151\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 24: 191.290039\n",
      "Minibatch accuracy: 32.5%\n",
      "Minibatch loss at step 26: 974.158081\n",
      "Minibatch accuracy: 32.5%\n",
      "Minibatch loss at step 28: 1288.448975\n",
      "Minibatch accuracy: 23.8%\n",
      "Minibatch loss at step 30: 1982.147705\n",
      "Minibatch accuracy: 42.5%\n",
      "Minibatch loss at step 32: 5808.711426\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 34: 8735.245117\n",
      "Minibatch accuracy: 42.5%\n",
      "Minibatch loss at step 36: 37667.628906\n",
      "Minibatch accuracy: 32.5%\n",
      "Minibatch loss at step 38: 34428.484375\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 40: 56789.460938\n",
      "Minibatch accuracy: 41.2%\n",
      "Minibatch loss at step 42: 145565.109375\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 44: 124466.453125\n",
      "Minibatch accuracy: 40.0%\n",
      "Minibatch loss at step 46: 189346.328125\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 48: 328532.281250\n",
      "Minibatch accuracy: 41.2%\n",
      "Minibatch loss at step 50: 577889.937500\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 52: 560473.625000\n",
      "Minibatch accuracy: 28.8%\n",
      "Minibatch loss at step 54: 944890.687500\n",
      "Minibatch accuracy: 27.5%\n",
      "Minibatch loss at step 56: 983031.562500\n",
      "Minibatch accuracy: 23.8%\n",
      "Minibatch loss at step 58: 1016392.000000\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 60: 1108330.875000\n",
      "Minibatch accuracy: 38.8%\n",
      "Minibatch loss at step 62: 961616.750000\n",
      "Minibatch accuracy: 48.8%\n",
      "Minibatch loss at step 64: 1913754.250000\n",
      "Minibatch accuracy: 36.2%\n",
      "Minibatch loss at step 66: 1490406.875000\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 68: 2247943.000000\n",
      "Minibatch accuracy: 20.0%\n",
      "Minibatch loss at step 70: 2441198.500000\n",
      "Minibatch accuracy: 28.8%\n",
      "Minibatch loss at step 72: 1949900.125000\n",
      "Minibatch accuracy: 18.8%\n",
      "Minibatch loss at step 74: 2961048.000000\n",
      "Minibatch accuracy: 32.5%\n",
      "Minibatch loss at step 76: 3685550.750000\n",
      "Minibatch accuracy: 36.2%\n",
      "Minibatch loss at step 78: 3713114.250000\n",
      "Minibatch accuracy: 36.2%\n",
      "Minibatch loss at step 80: 3737337.500000\n",
      "Minibatch accuracy: 33.8%\n",
      "Minibatch loss at step 82: 4201250.000000\n",
      "Minibatch accuracy: 45.0%\n",
      "Minibatch loss at step 84: 4013074.250000\n",
      "Minibatch accuracy: 30.0%\n",
      "Minibatch loss at step 86: 5100203.000000\n",
      "Minibatch accuracy: 20.0%\n",
      "Minibatch loss at step 88: 4531959.500000\n",
      "Minibatch accuracy: 41.2%\n",
      "Minibatch loss at step 90: 5590564.000000\n",
      "Minibatch accuracy: 31.2%\n",
      "Minibatch loss at step 92: 4825573.000000\n",
      "Minibatch accuracy: 22.5%\n",
      "Minibatch loss at step 94: 6015490.000000\n",
      "Minibatch accuracy: 37.5%\n",
      "Minibatch loss at step 96: 6613988.000000\n",
      "Minibatch accuracy: 36.2%\n",
      "Minibatch loss at step 98: 6253140.000000\n",
      "Minibatch accuracy: 36.2%\n",
      "Minibatch loss at step 100: 5955430.000000\n",
      "Minibatch accuracy: 30.0%\n",
      "Validation accuracy: 42.1%\n",
      "Test accuracy: 41.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_dataset_ref.shape[0] - batch_size)\n",
    "    feed_dict = dict()\n",
    "    batch_data = train_dataset_ref[offset:(offset + batch_size), :, :, :]\n",
    "    feed_dict[tf_train_dataset] = batch_data\n",
    "    batch_labels = list()\n",
    "    for i in range(max_length + 1):\n",
    "        batch_labels.append(train_labels_ref[i][offset:(offset + batch_size), :])\n",
    "        feed_dict[tf_train_labels[i]] = train_labels_ref[i][offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, l, predictions, train_n = session.run(\n",
    "      [optimizer, loss, train_prediction,train_num], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy_ref(train_n,predictions, t_l[offset:(offset + batch_size)]))\n",
    "    if (step % 100 == 0):\n",
    "      print('Validation accuracy: %.1f%%' % accuracy_ref(valid_num.eval(), valid_prediction.eval(), v_l))\n",
    "  print('Test accuracy: %.1f%%' % accuracy_ref(test_num.eval(),test_prediction.eval(), te_l))\n",
    "#   save_path = saver.save(session, \"CNN_multi.ckpt\")\n",
    "#   print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 28, 140, 1)\n"
     ]
    }
   ],
   "source": [
    "print train_dataset_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_one_hot(prob):\n",
    "    prob_one_hot= np.zeros(prob.shape[0])\n",
    "    prob_one_hot[np.argmax(prob)] = 1\n",
    "    return prob_one_hot\n",
    "    \n",
    "def accuracy_ref(predict_num, predictions, labels):\n",
    "    max_prob = []\n",
    "    final_prob = []\n",
    "    for i in predictions:\n",
    "        max_prob.append(np.max(i,axis=1))\n",
    "#     print max_prob\n",
    "    for i in range(4):\n",
    "        tmp = np.log(predict_num[:,i])\n",
    "        for j in range(i+1):\n",
    "#             print np.log(tmp)\n",
    "#             print np.log(max_prob[j])\n",
    "            tmp = tmp + np.log(max_prob[j])\n",
    "#             tmp = tmp * max_prob[j]\n",
    "        final_prob.append(tmp.tolist())\n",
    "    final_prob = np.array(final_prob).T\n",
    "#     print final_prob\n",
    "    final_index = np.argmax(final_prob,axis = 1)\n",
    "    print final_index\n",
    "    predict_labels = []\n",
    "    for probs in predictions:\n",
    "        predict_labels.append(np.argmax(probs, 1))\n",
    "    predict_labels = np.array(predict_labels).T\n",
    "    print predict_labels\n",
    "    for index, item in enumerate(final_index):\n",
    "        predict_labels[index , item + 1 :] = 11\n",
    "    print predict_labels\n",
    "    \n",
    "    return (100.0 * np.sum(predict_labels == labels)) / (len(predictions) * predictions[0].shape[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "def accuracy(predictions, labels):\n",
    "    accur = 0\n",
    "    for index, item in enumerate(predictions):\n",
    "        accur += np.sum(np.argmax(item, 1) == np.argmax(labels[index], 1))\n",
    "#         print accur\n",
    "#         print item.shape\n",
    "    return (100.0 * accur / float(len(predictions) * predictions[0].shape[0]))\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([[0.4,0.1,0.3,0.1,0.1],[0.2,0.9,0.1,0.1,0.1]])\n",
    "b = np.array([[0.1,0.2,0.3],[0.10,0.2,0.1]])\n",
    "c = np.array([[0.2,0.5,0.2],[0.20,0.1,0.2]])\n",
    "d = np.array([[2,1,4],[9,1,7]])\n",
    "e = np.array([[4,2,1],[0.1,0.4,0.1]])\n",
    "test = []\n",
    "test.append(b)\n",
    "test.append(c)\n",
    "test.append(d)\n",
    "test.append(e)\n",
    "test = np.array(test)\n",
    "label = np.array([[2,1,2,0],[1,11,11,11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0]\n",
      "[[2 1 2 0]\n",
      " [1 0 0 1]]\n",
      "[[ 2  1  2  0]\n",
      " [ 1 11 11 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_ref(a,test,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b[0] = prob_one_hot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [3, 2, 1]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_1 = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_1[2:,1,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [3, 2, 1]],\n",
       "\n",
       "       [[3, 2, 2],\n",
       "        [5, 4, 2]],\n",
       "\n",
       "       [[2, 1, 4],\n",
       "        [0, 0, 0]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis(=1) out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-fa8da72b18b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axis(=1) out of bounds"
     ]
    }
   ],
   "source": [
    "np.argmax([0,0,0],axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
